}else if(WLdatSource[j]=='KT_BE'){
WL_datList[[j]] = read.delim(WL_files[j],stringsAsFactors=F,sep=',',skip=3)
WL_datList[[j]]$TimeDate = as.POSIXct(paste0(WL_datList[[j]][,1],' ',WL_datList[[j]][,2]),format='%d.%m.%Y %H:%M',tz='GMT')
}else if(WLdatSource[j]=='KT_SO'){
WL_datList[[j]] = read.csv2(WL_files[j],stringsAsFactors=F,header = F)
WL_datList[[j]]$TimeDate = as.POSIXct(paste0(WL_datList[[j]][,1],' ',WL_datList[[j]][,2]),format='%d.%m.%Y %H:%M:%S',tz='GMT')
}else if(WLdatSource[j]=='CrowdWater'){
WL_datList[[j]] = read.delim(WL_files[j],sep=';',stringsAsFactors = F)
WL_datList[[j]]$TimeDate = as.POSIXct(WL_datList[[j]]$datetime,'%Y-%m-%d %H:%M:%S',tz='GMT',usetz=T)
}else if(WLdatSource[j]=='AT'){
WL_datList[[j]] = read.table(WL_files[j],sep='',stringsAsFactors = F,skip=26)
WL_datList[[j]]$TimeDate = as.POSIXct(paste0(WL_datList[[j]]$V1,' ',WL_datList[[j]]$V2),'%d.%m.%Y %H:%M:%S',tz='GMT',usetz=T)
}else if(WLdatSource[j]=='GKB'){
WL_datList[[j]] = read.table(WL_files[j],sep=';',stringsAsFactors = F,skip=1)
WL_datList[[j]]$TimeDate = as.POSIXct(WL_datList[[j]]$V1,'%Y-%m-%d %H:%M:%S',tz='GMT',usetz=T)
WL_datList[[j]]$V2[WL_datList[[j]]$V2<0] = NA # because the data was processed for HBV and has -9999 values
}else if(WLdatSource[j] == 'UNIL'){
WL_datList[[j]] = read.table(WL_files[j],sep=';',stringsAsFactors = F,header = T)
WL_datList[[j]]$TimeDate = as.POSIXct(WL_datList[[j]]$datetime,tz='GMT',usetz=T)
}
minDateList[j] = min(WL_datList[[j]]$TimeDate)
maxDateList[j] = max(WL_datList[[j]]$TimeDate)
}
# if the WL_datList contains the data of more than one station, trim it to the same time period (the longest thats available in both)
if(length(minDateList)>1){
for(j in seq_along(WL_datList)){
WL_datList[[j]] = WL_datList[[j]][WL_datList[[j]]$TimeDate>max(minDateList) & WL_datList[[j]]$TimeDate<min(maxDateList),]
}
}
# make a DF with all streamflow timeseries of all stations part of the location
t.WLdat_all = as.data.frame(matrix(nrow=nrow(WL_datList[[1]]),ncol=length(WLdatSource)))
for(j in seq_along(WLdatSource)){
if(WLdatSource[j]=='BAFU' || WLdatSource[j]=='KT_BE' || WLdatSource[j]=='KT_SO'|| WLdatSource[j]=='AT'){
t.WLdat_all[,j] = WL_datList[[j]][,3]
} else if(WLdatSource[j]=='KT_ZH'|| WLdatSource[j]=='GKB'|| WLdatSource[j]=='UNIL' || WLdatSource[j]=='CrowdWater'){
t.WLdat_all[,j] = WL_datList[[j]][,2]
}
}
for(j in seq_along(t.WLdat_all[1,])){
t.WLdat_all[,j] = suppressWarnings(as.numeric(t.WLdat_all[,j]))
}
# do the required calculation between all stations, i.e. over all rows of the the DF from the above codeblock
if(stationsWithData$WL_calc[i]=='add'){
Q = data.frame(rowSums(t.WLdat_all))
t.WLdat = data.frame(cbind(WL_datList[[1]]$TimeDate,Q),stringsAsFactors = F)
} else if(stationsWithData$WL_calc[i]=='average'){
Q = as.data.frame(rowMeans(t.WLdat_all) )
t.WLdat = data.frame(cbind(WL_datList[[1]]$TimeDate,Q),stringsAsFactors = F)
} else if(stationsWithData$WL_calc[i]==''){ # empty, no calculation needed (most cases are here)
t.WLdat = data.frame(cbind(WL_datList[[1]]$TimeDate,t.WLdat_all),stringsAsFactors = F)
}
colnames(t.WLdat) = c('datetimeCET','Waterlevel')
# Data from PnP stations ----
# read citsci estimates and rework data to proper format
# (CET is time noted by participants, GMT is equivalent time in GMT)
# is there data from pen & paper stations or from the app?
if(stationsWithData$DATA_FROM_PEN.PAPER[i]=='true'){
t.WLClassPnP = read.xlsx2(stationsWithData$CS_PenAndPaperPath[i],sheetIndex = 1,stringsAsFactors=F)
t.WLClassPnP = t.WLClassPnP[t.WLClassPnP$Datum != "",]
t.WLClassPnP$datetimeNmbrs = (as.numeric(t.WLClassPnP$Datum)+as.numeric(t.WLClassPnP$Uhrzeit))*60*60*24
t.WLClassPnP$datetimeCET = as.POSIXct(t.WLClassPnP$datetimeNmbrs,origin = '1899-12-30 00:00:00',tz='GMT')
t.WLClassPnP$Wasserstand = as.numeric(t.WLClassPnP$Wasserstand)
# remove citsci observations after the end of the Q-data
t.WLClassPnP = t.WLClassPnP[t.WLClassPnP$datetimeCET<=max(t.WLdat$datetimeCET),]
t.WLClassPnP = t.WLClassPnP[!is.na(t.WLClassPnP$Datum),]
# find Q measurements at times of WLclasses
t.WLClassPnP$MeasuredWL = NA
chartimes = as.character(t.WLdat$datetimeCET)
for(j in seq_along(t.WLClassPnP$datetimeCET)){
if(minute(t.WLClassPnP$datetimeCET[j])==0){
fullHQ = t.WLdat$Waterlevel[t.WLdat$datetimeCET==t.WLClassPnP$datetimeCET[j]]
# jump to next one if no Q data is available at the time of the observations
if(length(fullHQ)==0){next()}else{t.WLClassPnP$MeasuredWL[j] = fullHQ}
}else{
h.before = t.WLClassPnP$datetimeCET[j]-minute(t.WLClassPnP$datetimeCET[j])*60
h.after = t.WLClassPnP$datetimeCET[j]+(60-minute(t.WLClassPnP$datetimeCET[j]))*60
# find the Q before and after the WL-Class observation
q.before = t.WLdat$Waterlevel[chartimes == as.character(h.before)]
q.after = t.WLdat$Waterlevel[chartimes==as.character(h.after)]
# jump to next one if no Q data is available at the time of the observations
if(length(q.before)==0|length(q.after)==0)next()
# weights to multiply the q of the houre before and after with
weight.before = (60-minute(t.WLClassPnP$datetimeCET[j]))/60
weight.after = minute(t.WLClassPnP$datetimeCET[j])/60
t.WLClassPnP$MeasuredWL[j] = q.before*weight.before+q.after*weight.after
}
}
t.WLClassPnP_withWLData = t.WLClassPnP[!is.na(t.WLClassPnP$MeasuredWL) & !is.na(t.WLClassPnP$Wasserstand),]
# spearman rank correlation
SpearmanTest = wl_spearmanRankCorTest(data = t.WLClassPnP_withWLData, x="Wasserstand", y = "MeasuredWL")
# WL class test
t.WLClassPnP_withWLData$WLClasses_floor = floor(t.WLClassPnP_withWLData$Wasserstand)
# dunn.bonferroni test
DunnTestPlot(dataVar = t.WLClassPnP_withWLData,xcolumn="WLClasses_floor",ycolumn = "MeasuredWL",titleStr = paste0(stationsWithData$Stream[i],' - ',stationsWithData$Location[i],' [CrowdWater Forms]'),outFile=paste0(PlotFolder,'/Bonferroni_WL_',stationsWithData$Stream[i],'_',stationsWithData$Location[i],'_PenAndPaper.png'))
# spider graph
spiderChart(inputdata = t.WLClassPnP_withWLData$datetimeCET,nsegments=6,maxNcontribs = as.numeric(sneakyTab$maxContribPerHourPnP[i]),outFile = paste0(PlotFolder,'/SpiderChart_TOD_WL_',stationsWithData$Stream[i],'_',stationsWithData$Location[i],'_PenAndPaper.png'),titleStr = paste0(stationsWithData$Stream[i],' - ',stationsWithData$Location[i],' [CrowdWater Forms]'))
maxContribPerHourPnPList[i] = maxContribPerHour
spiderChart_year(inputdata = t.WLClassPnP_withWLData$datetimeCET,nsegments=6,maxNcontribs = as.numeric(sneakyTab$maxContribPerWeekOvrl[i]),outFile = paste0(PlotFolder,'/SpiderChart_Week_WL_',stationsWithData$Stream[i],'_',stationsWithData$Location[i],'_PenAndPaper.png'),titleStr = paste0(stationsWithData$Stream[i],' - ',stationsWithData$Location[i],' [CrowdWater Forms]'))
maxContribPerWeekOvrlList[i] = maxContribPerWeek
# add data to grand table
t.uq.classes = unique(t.WLClassPnP_withWLData$Wasserstand)
stationsWithData$NrPoints_Form[i] = length(t.WLClassPnP_withWLData$Wasserstand)
stationsWithData$NrDiffPpl_Form[i] = nrow(t.WLClassPnP_withWLData)-sum(as.numeric(t.WLClassPnP_withWLData$Wie.oft.),na.rm = T)
stationsWithData$NrClassesCvrd_Form[i] = length(t.uq.classes)
stationsWithData$SpearmanRho_Form[i] = SpearmanTest$estimate
stationsWithData$SpearmanP_Form[i] = SpearmanTest$p.value
# create the boxplots
wlclass_plot(outFile = paste0(PlotFolder,'/Boxplot_WL_',stationsWithData$Stream[i],'_',stationsWithData$Location[i],'_PenAndPaper.png'),
dataVar = t.WLClassPnP_withWLData, xcolumn="WLClasses_floor", ycolumn = "MeasuredWL",
xAxlabel = "WL-Class", yAxlabel=paste0('Measured Waterlevel [',stationsWithData$WL_unit[i],']'),
titleStr = paste0(stationsWithData$Stream[i],' - ',stationsWithData$Location[i],' [CrowdWater Forms]'),
spearmanRho = SpearmanTest$estimate, spearmanP = SpearmanTest$p.value,nContributors = stationsWithData$NrDiffPpl_Form[i])
rm(t.uq.classes,SpearmanTest,maxContribPerHour)
}
# Data from CW App ----
#### if there is data from the CW app
if(stationsWithData$DATA_FROM_CWAPP[i]=='true'){
t.WLClassApp = read.csv2(stationsWithData$CS_AppDataPath[i],stringsAsFactors=F)
t.WLClassApp = t.WLClassApp[t.WLClassApp$ROOT_ID==stationsWithData$RootID[i],]
t.WLClassApp$datetimeCET = as.POSIXct(t.WLClassApp$CREATED_AT,format='%Y-%m-%d %H:%M', tz='GMT') + 3600 # app time is in GMT directly, so add 1 hour to get CET without DST
t.WLClassApp$RealDateTime = as.POSIXct(t.WLClassApp$SPOTTED_AT,format='%Y-%m-%d %H:%M', tz='CET')  # This is the local time of the user, in the case of this paper its all CET
t.WLClassApp$WATER_LEVEL = unname(as.numeric(unlist(sapply(t.WLClassApp$WATER_LEVEL,function(x) AppWLLUT$WLnumber[AppWLLUT$appCat==x]))))
# remove citsci observations with less after the end of the Q-data
t.WLClassApp = t.WLClassApp[t.WLClassApp$datetimeCET<=max(t.WLdat$datetimeCET),]
t.WLClassApp = t.WLClassApp[!is.na(t.WLClassApp$datetimeCET),]
# find Q measurements at times of WLclasses
t.WLClassApp$MeasuredWL = NA
t.chartimes = as.character(t.WLdat$datetimeCET)
for(j in seq_along(t.WLClassApp$datetimeCET)){
if(minute(t.WLClassApp$datetimeCET[j])==0){
fullHQ = t.WLdat$Waterlevel[t.WLdat$datetimeCET==t.WLClassApp$datetimeCET[j]]
# jump to next one if no Q data is available at the time of the observations
if(length(fullHQ)==0){next()}else{t.WLClassApp$MeasuredWL[j] = fullHQ}
}else{
h.before = t.WLClassApp$datetimeCET[j]-minute(t.WLClassApp$datetimeCET[j])*60
h.after = t.WLClassApp$datetimeCET[j]+(60-minute(t.WLClassApp$datetimeCET[j]))*60
# find the Q before and after the WL-Class observation
q.before = t.WLdat$Waterlevel[t.chartimes==as.character(h.before)]
q.after = t.WLdat$Waterlevel[t.chartimes==as.character(h.after)]
if(length(q.before)==0|length(q.after)==0)next()
# weights to multiply the q of the houre before and after with
weight.before = (60-minute(t.WLClassApp$datetimeCET[j]))/60
weight.after = minute(t.WLClassApp$datetimeCET[j])/60
t.WLClassApp$MeasuredWL[j] = q.before*weight.before+q.after*weight.after
}
}
t.WLClassApp_WithWLData = t.WLClassApp[!is.na(t.WLClassApp$WATER_LEVEL) & !is.na(t.WLClassApp$MeasuredWL),]
# rank correlation test (spearman's R)
# http://www.sthda.com/english/wiki/correlation-test-between-two-variables-in-r#spearman-correlation-formula
# shapiro.test(t.WLClassApp$WATER_LEVEL) --> not normally distributed --> spearman instead of pearsons r
# shapiro.test(t.WLClassApp$MeasuredWL)--> not normally distributed --> spearman instead of pearsons r
# spearman rank correlation
SpearmanTest = wl_spearmanRankCorTest(data = t.WLClassApp_WithWLData, x="WATER_LEVEL", y = "MeasuredWL")
# WL class test
t.WLClassApp_WithWLData$WLClasses_floor = floor(t.WLClassApp_WithWLData$WATER_LEVEL)
# dunn.bonferroni test
DunnTestPlot(dataVar = t.WLClassApp_WithWLData,xcolumn="WLClasses_floor",ycolumn = "MeasuredWL",titleStr = paste0(stationsWithData$Stream[i],' - ',stationsWithData$Location[i],' [CrowdWater App]'),outFile=paste0(PlotFolder,'/Bonferroni_WL_',stationsWithData$Stream[i],'_',stationsWithData$Location[i],'_CWApp.png'))
# spider graph
spiderChart(inputdata = t.WLClassApp_WithWLData$RealDateTime,nsegments = 6,maxNcontribs=as.numeric(sneakyTab$maxContribPerHourApp[i]),outFile = paste0(PlotFolder,'/SpiderChart_TOD_WL_',stationsWithData$Stream[i],'_',stationsWithData$Location[i],'_CWApp.png'),titleStr = paste0(stationsWithData$Stream[i],' - ',stationsWithData$Location[i],' [CrowdWater App]'))
spiderChart_year(inputdata = t.WLClassApp_WithWLData$RealDateTime,nsegments = 6,maxNcontribs=as.numeric(sneakyTab$maxContribPerWeekApp[i]),outFile = paste0(PlotFolder,'/SpiderChart_Week_WL_',stationsWithData$Stream[i],'_',stationsWithData$Location[i],'_CWApp.png'),titleStr = paste0(stationsWithData$Stream[i],' - ',stationsWithData$Location[i],' [CrowdWater App]'))
maxContribPerHourAppList[i] = maxContribPerHour
maxContribPerWeekAppList[i] = maxContribPerWeek
# add data to grand table
t.uq.classes = unique(t.WLClassApp_WithWLData$WATER_LEVEL)
stationsWithData$NrPoints_App[i] = length(t.WLClassApp_WithWLData$WATER_LEVEL)
stationsWithData$NrDiffPpl_App[i] = length(unique(t.WLClassApp_WithWLData$USER_ID))
stationsWithData$NrClassesCvrd_App[i] = length(t.uq.classes)
stationsWithData$SpearmanRho_App[i] = SpearmanTest$estimate
stationsWithData$SpearmanP_App[i] = SpearmanTest$p.value
# create the boxplot
wlclass_plot(outFile = paste0(PlotFolder,'/Boxplot_WL_',stationsWithData$Stream[i],'_',stationsWithData$Location[i],'_CWApp.png'),
dataVar = t.WLClassApp_WithWLData, xcolumn="WLClasses_floor", ycolumn = "MeasuredWL",
xAxlabel = "WL-Class", yAxlabel=paste0('Measured Waterlevel [',stationsWithData$WL_unit[i],']'),
titleStr = paste0(stationsWithData$Stream[i],' - ',stationsWithData$Location[i],' [CrowdWater App]'),
spearmanRho = SpearmanTest$estimate, spearmanP = SpearmanTest$p.value,nContributors = stationsWithData$NrDiffPpl_App[i])
# check if people improved ----
# if there are 10 or more contributions by the same person
userContribNr = table(t.WLClassApp_WithWLData$USER_ID)
usersWithMoreThanEq10Contribs = names(userContribNr)[userContribNr>=15]
if(length(usersWithMoreThanEq10Contribs)==0){
}else{
for(tra in 1: seq_along(usersWithMoreThanEq10Contribs)){
t.usrContribs = t.WLClassApp_WithWLData[t.WLClassApp_WithWLData$USER_ID==usersWithMoreThanEq10Contribs[tra],]
t.usrContribs$fstOr2ndhalf = NA
spearTests1stHalf = vector()
spearTests2ndHalf = vector()
#iterate along the entire set of contributions and always add one more contribution and perform the spearman
# rank test on both parts of the set
splitters = 2:(nrow(t.usrContribs)-2)
for(ii in splitters){ # from 2 and until nrow()-2 to avoid having only one value
t.usrContribs$fstOr2ndhalf[1:ii] = 'Second half'
t.usrContribs$fstOr2ndhalf[(ii+1):nrow(t.usrContribs)] = 'First half'
SpearmanTest_1sthalf = wl_spearmanRankCorTest(data = t.usrContribs[t.usrContribs$fstOr2ndhalf=='First half',], x="WATER_LEVEL", y = "MeasuredWL")
SpearmanTest_2ndhalf = wl_spearmanRankCorTest(data = t.usrContribs[t.usrContribs$fstOr2ndhalf=='Second half',], x="WATER_LEVEL", y = "MeasuredWL")
spearTests1stHalf[ii] = SpearmanTest_1sthalf$estimate
spearTests2ndHalf[ii] = SpearmanTest_2ndhalf$estimate
}
# reverse the series to make it more intuitive
spearTests1stHalf = rev(spearTests1stHalf)
spearTests2ndHalf = rev(spearTests2ndHalf)
# bp = breakpoints(spearTests1stHalf~1)
# smoothing the data ----
windowSize = 5
smooth1stHalf = rollapply(spearTests1stHalf, width = windowSize, FUN = mean, align = "left")
smooth2ndHalf = rollapply(spearTests2ndHalf, width = windowSize, FUN = mean, align = "left")
# Check where the difference is biggest within the 10-90 percent of the data
smooth1stHalf_10t090 = smooth1stHalf[((length(smooth1stHalf)/10)*2):((length(smooth1stHalf)/10)*8)]
smooth2ndHalf_10t090 = smooth2ndHalf[((length(smooth2ndHalf)/10)*2):((length(smooth2ndHalf)/10)*8)]
absDifsOfSmooths = abs(smooth1stHalf-smooth2ndHalf)
which(max(absDifsOfSmooths,na.rm=T) == absDifsOfSmooths)
# code below only useful for testing with multiple splits for the dataset (default is 2 for splitting the data in half)
png(paste0('G:/h2k-data/Projects/CrowdWater/WP7 - CW Field and App Data Accuracy/Data/DataVisualisations/',stationsWithData$Stream[i],' - ',stationsWithData$Location[i],' User',tra,'.png'),width = 1024,height = 720)
plot(y=spearTests1stHalf,x=seq_along(spearTests1stHalf),col='orange',type='l',ylim=c(min(c(spearTests1stHalf,spearTests2ndHalf),na.rm = T),max(c(spearTests1stHalf,spearTests2ndHalf),na.rm = T)),main= paste0(stationsWithData$Stream[i],' - ',stationsWithData$Location[i],' [CrowdWater App]'),ylab= 'spearmans rho',xlab='Number of contribution [low numbers: almost no values are in the first part]')
lines(y=spearTests2ndHalf,x=seq_along(spearTests2ndHalf),col='lightblue')
lines(y=smooth1stHalf_10t090,x=seq_along(smooth1stHalf_10t090),col='red')
lines(y=smooth2ndHalf_10t090,x=seq_along(smooth2ndHalf_10t090),col='blue')
# abline(v=bp$breakpoints,col='darkgrey')
# text(labels=paste0('First BreakPoint - ', max(bp$breakpoints)),x =max(bp$breakpoints) ,y=mean(smooth1stHalf,na.rm = T))
legend(x='bottom',legend=c('1st half','1st half smooth','2nd half','2nd half smooth','Breakp. 1st H.'),fill = c('red','orange','blue','lightblue','darkgrey'))
dev.off()
t.usrContribs$WL4Plot = NA
t.usrContribs$WL4Plot[t.usrContribs$fstOr2ndhalf=='Second half'] = t.usrContribs$WLClasses_floor[t.usrContribs$fstOr2ndhalf=='Second half'] + 0.15
t.usrContribs$WL4Plot[t.usrContribs$fstOr2ndhalf=='First half'] = t.usrContribs$WLClasses_floor[t.usrContribs$fstOr2ndhalf=='First half'] - 0.15
# create the boxplot
png(paste0('G:/h2k-data/Projects/CrowdWater/WP7 - CW Field and App Data Accuracy/Data/DataVisualisations/Boxplot_WL_',stationsWithData$Stream[i],'_',stationsWithData$Location[i],'_CWApp.png'),width = 1024,height = 720)
print(
ggplot(data=t.usrContribs,aes(x=WL4Plot,y=MeasuredWL,fill=fstOr2ndhalf,group=WL4Plot))+
geom_boxplot())+
ggtitle(paste0(usersWithMoreThanEq10Contribs[tra],' - ',stationsWithData$Stream[i],' - ',stationsWithData$Location[i],' [CrowdWater App]'))
dev.off()
}
}
# split their contribution in half and do two additional boxplots for that person
rm(t.uq.classes,SpearmanTest,maxContribPerHour)
}
source('G:/h2k-data/Projects/CrowdWater/WP7 - CW Field and App Data Accuracy/Rscripts/Correlation_WLClassEstimates_WaterlevelData.R', echo=TRUE)
source('G:/h2k-data/Projects/CrowdWater/WP7 - CW Field and App Data Accuracy/Rscripts/Correlation_WLClassEstimates_WaterlevelData.R', echo=TRUE)
which(max(absDifsOfSmooths,na.rm=T) == absDifsOfSmooths)
i
stationsWithData$Stream
plot(y=spearTests1stHalf,x=seq_along(spearTests1stHalf),col='orange',type='l',ylim=c(min(c(spearTests1stHalf,spearTests2ndHalf),na.rm = T),max(c(spearTests1stHalf,spearTests2ndHalf),na.rm = T)),main= paste0(stationsWithData$Stream[i],' - ',stationsWithData$Location[i],' [CrowdWater App]'),ylab= 'spearmans rho',xlab='Number of contribution [low numbers: almost no values are in the first part]')
lines(y=spearTests2ndHalf,x=seq_along(spearTests2ndHalf),col='lightblue')
lines(y=smooth1stHalf_10t090,x=seq_along(smooth1stHalf_10t090),col='red')
lines(y=smooth2ndHalf_10t090,x=seq_along(smooth2ndHalf_10t090),col='blue')
# Check where the difference is biggest within the 10-90 percent of the data
smooth1stHalf_10t090 = smooth1stHalf[((length(smooth1stHalf)/10)*2):((length(smooth1stHalf)/10)*8)]
smooth2ndHalf_10t090 = smooth2ndHalf[((length(smooth2ndHalf)/10)*2):((length(smooth2ndHalf)/10)*8)]
WL_files = unlist(strsplit(stationsWithData$WL_hourly_DataPath[i],' AND '))
WL_datList = list()
minDateList = as.POSIXct(vector()) # date vector for finding minimum dates if >1 Q files available
maxDateList = as.POSIXct(vector()) # date vector for finding maximum dates if >1 Q files available
# read in the files ----
WLdatSource = vector()
for(j in seq_along(WL_files)){
WLdatSource[j] = strsplit(stationsWithData$QDataSource[i],' AND ')[[1]][j]
# if it is a BAFU-File (.asc)
if(WLdatSource[j]=='BAFU'){
WL_datList[[j]] = read.delim(WL_files[j],sep=';', stringsAsFactors = F,skip=3,header=F)
WL_datList[[j]]$TimeDate = as.POSIXct(sapply(strsplit(WL_datList[[j]]$V2,'-'),'[[',2),format='%Y.%m.%d %H:%M', tz='GMT',usetz=T)
} else if(WLdatSource[j]=='KT_ZH'){
WL_datList[[j]] = read.delim(WL_files[j],sep=';', stringsAsFactors = F,skip=0,header=T)
WL_datList[[j]]$TimeDate = as.POSIXct(WL_datList[[j]][,1],format='%Y-%m-%d %H:%M:%S',tz='GMT')
}else if(WLdatSource[j]=='KT_BE'){
WL_datList[[j]] = read.delim(WL_files[j],stringsAsFactors=F,sep=',',skip=3)
WL_datList[[j]]$TimeDate = as.POSIXct(paste0(WL_datList[[j]][,1],' ',WL_datList[[j]][,2]),format='%d.%m.%Y %H:%M',tz='GMT')
}else if(WLdatSource[j]=='KT_SO'){
WL_datList[[j]] = read.csv2(WL_files[j],stringsAsFactors=F,header = F)
WL_datList[[j]]$TimeDate = as.POSIXct(paste0(WL_datList[[j]][,1],' ',WL_datList[[j]][,2]),format='%d.%m.%Y %H:%M:%S',tz='GMT')
}else if(WLdatSource[j]=='CrowdWater'){
WL_datList[[j]] = read.delim(WL_files[j],sep=';',stringsAsFactors = F)
WL_datList[[j]]$TimeDate = as.POSIXct(WL_datList[[j]]$datetime,'%Y-%m-%d %H:%M:%S',tz='GMT',usetz=T)
}else if(WLdatSource[j]=='AT'){
WL_datList[[j]] = read.table(WL_files[j],sep='',stringsAsFactors = F,skip=26)
WL_datList[[j]]$TimeDate = as.POSIXct(paste0(WL_datList[[j]]$V1,' ',WL_datList[[j]]$V2),'%d.%m.%Y %H:%M:%S',tz='GMT',usetz=T)
}else if(WLdatSource[j]=='GKB'){
WL_datList[[j]] = read.table(WL_files[j],sep=';',stringsAsFactors = F,skip=1)
WL_datList[[j]]$TimeDate = as.POSIXct(WL_datList[[j]]$V1,'%Y-%m-%d %H:%M:%S',tz='GMT',usetz=T)
WL_datList[[j]]$V2[WL_datList[[j]]$V2<0] = NA # because the data was processed for HBV and has -9999 values
}else if(WLdatSource[j] == 'UNIL'){
WL_datList[[j]] = read.table(WL_files[j],sep=';',stringsAsFactors = F,header = T)
WL_datList[[j]]$TimeDate = as.POSIXct(WL_datList[[j]]$datetime,tz='GMT',usetz=T)
}
minDateList[j] = min(WL_datList[[j]]$TimeDate)
maxDateList[j] = max(WL_datList[[j]]$TimeDate)
}
# if the WL_datList contains the data of more than one station, trim it to the same time period (the longest thats available in both)
if(length(minDateList)>1){
for(j in seq_along(WL_datList)){
WL_datList[[j]] = WL_datList[[j]][WL_datList[[j]]$TimeDate>max(minDateList) & WL_datList[[j]]$TimeDate<min(maxDateList),]
}
}
# make a DF with all streamflow timeseries of all stations part of the location
t.WLdat_all = as.data.frame(matrix(nrow=nrow(WL_datList[[1]]),ncol=length(WLdatSource)))
for(j in seq_along(WLdatSource)){
if(WLdatSource[j]=='BAFU' || WLdatSource[j]=='KT_BE' || WLdatSource[j]=='KT_SO'|| WLdatSource[j]=='AT'){
t.WLdat_all[,j] = WL_datList[[j]][,3]
} else if(WLdatSource[j]=='KT_ZH'|| WLdatSource[j]=='GKB'|| WLdatSource[j]=='UNIL' || WLdatSource[j]=='CrowdWater'){
t.WLdat_all[,j] = WL_datList[[j]][,2]
}
}
for(j in seq_along(t.WLdat_all[1,])){
t.WLdat_all[,j] = suppressWarnings(as.numeric(t.WLdat_all[,j]))
}
# do the required calculation between all stations, i.e. over all rows of the the DF from the above codeblock
if(stationsWithData$WL_calc[i]=='add'){
Q = data.frame(rowSums(t.WLdat_all))
t.WLdat = data.frame(cbind(WL_datList[[1]]$TimeDate,Q),stringsAsFactors = F)
} else if(stationsWithData$WL_calc[i]=='average'){
Q = as.data.frame(rowMeans(t.WLdat_all) )
t.WLdat = data.frame(cbind(WL_datList[[1]]$TimeDate,Q),stringsAsFactors = F)
} else if(stationsWithData$WL_calc[i]==''){ # empty, no calculation needed (most cases are here)
t.WLdat = data.frame(cbind(WL_datList[[1]]$TimeDate,t.WLdat_all),stringsAsFactors = F)
}
colnames(t.WLdat) = c('datetimeCET','Waterlevel')
# Data from PnP stations ----
# read citsci estimates and rework data to proper format
# (CET is time noted by participants, GMT is equivalent time in GMT)
# is there data from pen & paper stations or from the app?
if(stationsWithData$DATA_FROM_PEN.PAPER[i]=='true'){
t.WLClassPnP = read.xlsx2(stationsWithData$CS_PenAndPaperPath[i],sheetIndex = 1,stringsAsFactors=F)
t.WLClassPnP = t.WLClassPnP[t.WLClassPnP$Datum != "",]
t.WLClassPnP$datetimeNmbrs = (as.numeric(t.WLClassPnP$Datum)+as.numeric(t.WLClassPnP$Uhrzeit))*60*60*24
t.WLClassPnP$datetimeCET = as.POSIXct(t.WLClassPnP$datetimeNmbrs,origin = '1899-12-30 00:00:00',tz='GMT')
t.WLClassPnP$Wasserstand = as.numeric(t.WLClassPnP$Wasserstand)
# remove citsci observations after the end of the Q-data
t.WLClassPnP = t.WLClassPnP[t.WLClassPnP$datetimeCET<=max(t.WLdat$datetimeCET),]
t.WLClassPnP = t.WLClassPnP[!is.na(t.WLClassPnP$Datum),]
# find Q measurements at times of WLclasses
t.WLClassPnP$MeasuredWL = NA
chartimes = as.character(t.WLdat$datetimeCET)
for(j in seq_along(t.WLClassPnP$datetimeCET)){
if(minute(t.WLClassPnP$datetimeCET[j])==0){
fullHQ = t.WLdat$Waterlevel[t.WLdat$datetimeCET==t.WLClassPnP$datetimeCET[j]]
# jump to next one if no Q data is available at the time of the observations
if(length(fullHQ)==0){next()}else{t.WLClassPnP$MeasuredWL[j] = fullHQ}
}else{
h.before = t.WLClassPnP$datetimeCET[j]-minute(t.WLClassPnP$datetimeCET[j])*60
h.after = t.WLClassPnP$datetimeCET[j]+(60-minute(t.WLClassPnP$datetimeCET[j]))*60
# find the Q before and after the WL-Class observation
q.before = t.WLdat$Waterlevel[chartimes == as.character(h.before)]
q.after = t.WLdat$Waterlevel[chartimes==as.character(h.after)]
# jump to next one if no Q data is available at the time of the observations
if(length(q.before)==0|length(q.after)==0)next()
# weights to multiply the q of the houre before and after with
weight.before = (60-minute(t.WLClassPnP$datetimeCET[j]))/60
weight.after = minute(t.WLClassPnP$datetimeCET[j])/60
t.WLClassPnP$MeasuredWL[j] = q.before*weight.before+q.after*weight.after
}
}
t.WLClassPnP_withWLData = t.WLClassPnP[!is.na(t.WLClassPnP$MeasuredWL) & !is.na(t.WLClassPnP$Wasserstand),]
# spearman rank correlation
SpearmanTest = wl_spearmanRankCorTest(data = t.WLClassPnP_withWLData, x="Wasserstand", y = "MeasuredWL")
# WL class test
t.WLClassPnP_withWLData$WLClasses_floor = floor(t.WLClassPnP_withWLData$Wasserstand)
# dunn.bonferroni test
DunnTestPlot(dataVar = t.WLClassPnP_withWLData,xcolumn="WLClasses_floor",ycolumn = "MeasuredWL",titleStr = paste0(stationsWithData$Stream[i],' - ',stationsWithData$Location[i],' [CrowdWater Forms]'),outFile=paste0(PlotFolder,'/Bonferroni_WL_',stationsWithData$Stream[i],'_',stationsWithData$Location[i],'_PenAndPaper.png'))
# spider graph
spiderChart(inputdata = t.WLClassPnP_withWLData$datetimeCET,nsegments=6,maxNcontribs = as.numeric(sneakyTab$maxContribPerHourPnP[i]),outFile = paste0(PlotFolder,'/SpiderChart_TOD_WL_',stationsWithData$Stream[i],'_',stationsWithData$Location[i],'_PenAndPaper.png'),titleStr = paste0(stationsWithData$Stream[i],' - ',stationsWithData$Location[i],' [CrowdWater Forms]'))
maxContribPerHourPnPList[i] = maxContribPerHour
spiderChart_year(inputdata = t.WLClassPnP_withWLData$datetimeCET,nsegments=6,maxNcontribs = as.numeric(sneakyTab$maxContribPerWeekOvrl[i]),outFile = paste0(PlotFolder,'/SpiderChart_Week_WL_',stationsWithData$Stream[i],'_',stationsWithData$Location[i],'_PenAndPaper.png'),titleStr = paste0(stationsWithData$Stream[i],' - ',stationsWithData$Location[i],' [CrowdWater Forms]'))
maxContribPerWeekOvrlList[i] = maxContribPerWeek
# add data to grand table
t.uq.classes = unique(t.WLClassPnP_withWLData$Wasserstand)
stationsWithData$NrPoints_Form[i] = length(t.WLClassPnP_withWLData$Wasserstand)
stationsWithData$NrDiffPpl_Form[i] = nrow(t.WLClassPnP_withWLData)-sum(as.numeric(t.WLClassPnP_withWLData$Wie.oft.),na.rm = T)
stationsWithData$NrClassesCvrd_Form[i] = length(t.uq.classes)
stationsWithData$SpearmanRho_Form[i] = SpearmanTest$estimate
stationsWithData$SpearmanP_Form[i] = SpearmanTest$p.value
# create the boxplots
wlclass_plot(outFile = paste0(PlotFolder,'/Boxplot_WL_',stationsWithData$Stream[i],'_',stationsWithData$Location[i],'_PenAndPaper.png'),
dataVar = t.WLClassPnP_withWLData, xcolumn="WLClasses_floor", ycolumn = "MeasuredWL",
xAxlabel = "WL-Class", yAxlabel=paste0('Measured Waterlevel [',stationsWithData$WL_unit[i],']'),
titleStr = paste0(stationsWithData$Stream[i],' - ',stationsWithData$Location[i],' [CrowdWater Forms]'),
spearmanRho = SpearmanTest$estimate, spearmanP = SpearmanTest$p.value,nContributors = stationsWithData$NrDiffPpl_Form[i])
rm(t.uq.classes,SpearmanTest,maxContribPerHour)
}
# Data from CW App ----
#### if there is data from the CW app
if(stationsWithData$DATA_FROM_CWAPP[i]=='true'){
t.WLClassApp = read.csv2(stationsWithData$CS_AppDataPath[i],stringsAsFactors=F)
t.WLClassApp = t.WLClassApp[t.WLClassApp$ROOT_ID==stationsWithData$RootID[i],]
t.WLClassApp$datetimeCET = as.POSIXct(t.WLClassApp$CREATED_AT,format='%Y-%m-%d %H:%M', tz='GMT') + 3600 # app time is in GMT directly, so add 1 hour to get CET without DST
t.WLClassApp$RealDateTime = as.POSIXct(t.WLClassApp$SPOTTED_AT,format='%Y-%m-%d %H:%M', tz='CET')  # This is the local time of the user, in the case of this paper its all CET
t.WLClassApp$WATER_LEVEL = unname(as.numeric(unlist(sapply(t.WLClassApp$WATER_LEVEL,function(x) AppWLLUT$WLnumber[AppWLLUT$appCat==x]))))
# remove citsci observations with less after the end of the Q-data
t.WLClassApp = t.WLClassApp[t.WLClassApp$datetimeCET<=max(t.WLdat$datetimeCET),]
t.WLClassApp = t.WLClassApp[!is.na(t.WLClassApp$datetimeCET),]
# find Q measurements at times of WLclasses
t.WLClassApp$MeasuredWL = NA
t.chartimes = as.character(t.WLdat$datetimeCET)
for(j in seq_along(t.WLClassApp$datetimeCET)){
if(minute(t.WLClassApp$datetimeCET[j])==0){
fullHQ = t.WLdat$Waterlevel[t.WLdat$datetimeCET==t.WLClassApp$datetimeCET[j]]
# jump to next one if no Q data is available at the time of the observations
if(length(fullHQ)==0){next()}else{t.WLClassApp$MeasuredWL[j] = fullHQ}
}else{
h.before = t.WLClassApp$datetimeCET[j]-minute(t.WLClassApp$datetimeCET[j])*60
h.after = t.WLClassApp$datetimeCET[j]+(60-minute(t.WLClassApp$datetimeCET[j]))*60
# find the Q before and after the WL-Class observation
q.before = t.WLdat$Waterlevel[t.chartimes==as.character(h.before)]
q.after = t.WLdat$Waterlevel[t.chartimes==as.character(h.after)]
if(length(q.before)==0|length(q.after)==0)next()
# weights to multiply the q of the houre before and after with
weight.before = (60-minute(t.WLClassApp$datetimeCET[j]))/60
weight.after = minute(t.WLClassApp$datetimeCET[j])/60
t.WLClassApp$MeasuredWL[j] = q.before*weight.before+q.after*weight.after
}
}
t.WLClassApp_WithWLData = t.WLClassApp[!is.na(t.WLClassApp$WATER_LEVEL) & !is.na(t.WLClassApp$MeasuredWL),]
# rank correlation test (spearman's R)
# http://www.sthda.com/english/wiki/correlation-test-between-two-variables-in-r#spearman-correlation-formula
# shapiro.test(t.WLClassApp$WATER_LEVEL) --> not normally distributed --> spearman instead of pearsons r
# shapiro.test(t.WLClassApp$MeasuredWL)--> not normally distributed --> spearman instead of pearsons r
# spearman rank correlation
SpearmanTest = wl_spearmanRankCorTest(data = t.WLClassApp_WithWLData, x="WATER_LEVEL", y = "MeasuredWL")
# WL class test
t.WLClassApp_WithWLData$WLClasses_floor = floor(t.WLClassApp_WithWLData$WATER_LEVEL)
# dunn.bonferroni test
DunnTestPlot(dataVar = t.WLClassApp_WithWLData,xcolumn="WLClasses_floor",ycolumn = "MeasuredWL",titleStr = paste0(stationsWithData$Stream[i],' - ',stationsWithData$Location[i],' [CrowdWater App]'),outFile=paste0(PlotFolder,'/Bonferroni_WL_',stationsWithData$Stream[i],'_',stationsWithData$Location[i],'_CWApp.png'))
# spider graph
spiderChart(inputdata = t.WLClassApp_WithWLData$RealDateTime,nsegments = 6,maxNcontribs=as.numeric(sneakyTab$maxContribPerHourApp[i]),outFile = paste0(PlotFolder,'/SpiderChart_TOD_WL_',stationsWithData$Stream[i],'_',stationsWithData$Location[i],'_CWApp.png'),titleStr = paste0(stationsWithData$Stream[i],' - ',stationsWithData$Location[i],' [CrowdWater App]'))
spiderChart_year(inputdata = t.WLClassApp_WithWLData$RealDateTime,nsegments = 6,maxNcontribs=as.numeric(sneakyTab$maxContribPerWeekApp[i]),outFile = paste0(PlotFolder,'/SpiderChart_Week_WL_',stationsWithData$Stream[i],'_',stationsWithData$Location[i],'_CWApp.png'),titleStr = paste0(stationsWithData$Stream[i],' - ',stationsWithData$Location[i],' [CrowdWater App]'))
maxContribPerHourAppList[i] = maxContribPerHour
maxContribPerWeekAppList[i] = maxContribPerWeek
# add data to grand table
t.uq.classes = unique(t.WLClassApp_WithWLData$WATER_LEVEL)
stationsWithData$NrPoints_App[i] = length(t.WLClassApp_WithWLData$WATER_LEVEL)
stationsWithData$NrDiffPpl_App[i] = length(unique(t.WLClassApp_WithWLData$USER_ID))
stationsWithData$NrClassesCvrd_App[i] = length(t.uq.classes)
stationsWithData$SpearmanRho_App[i] = SpearmanTest$estimate
stationsWithData$SpearmanP_App[i] = SpearmanTest$p.value
# create the boxplot
wlclass_plot(outFile = paste0(PlotFolder,'/Boxplot_WL_',stationsWithData$Stream[i],'_',stationsWithData$Location[i],'_CWApp.png'),
dataVar = t.WLClassApp_WithWLData, xcolumn="WLClasses_floor", ycolumn = "MeasuredWL",
xAxlabel = "WL-Class", yAxlabel=paste0('Measured Waterlevel [',stationsWithData$WL_unit[i],']'),
titleStr = paste0(stationsWithData$Stream[i],' - ',stationsWithData$Location[i],' [CrowdWater App]'),
spearmanRho = SpearmanTest$estimate, spearmanP = SpearmanTest$p.value,nContributors = stationsWithData$NrDiffPpl_App[i])
# check if people improved ----
# if there are 10 or more contributions by the same person
userContribNr = table(t.WLClassApp_WithWLData$USER_ID)
usersWithMoreThanEq10Contribs = names(userContribNr)[userContribNr>=15]
if(length(usersWithMoreThanEq10Contribs)==0){
}else{
for(tra in 1: seq_along(usersWithMoreThanEq10Contribs)){
t.usrContribs = t.WLClassApp_WithWLData[t.WLClassApp_WithWLData$USER_ID==usersWithMoreThanEq10Contribs[tra],]
t.usrContribs$fstOr2ndhalf = NA
spearTests1stHalf = vector()
spearTests2ndHalf = vector()
#iterate along the entire set of contributions and always add one more contribution and perform the spearman
# rank test on both parts of the set
splitters = 2:(nrow(t.usrContribs)-2)
for(ii in splitters){ # from 2 and until nrow()-2 to avoid having only one value
t.usrContribs$fstOr2ndhalf[1:ii] = 'Second half'
t.usrContribs$fstOr2ndhalf[(ii+1):nrow(t.usrContribs)] = 'First half'
SpearmanTest_1sthalf = wl_spearmanRankCorTest(data = t.usrContribs[t.usrContribs$fstOr2ndhalf=='First half',], x="WATER_LEVEL", y = "MeasuredWL")
SpearmanTest_2ndhalf = wl_spearmanRankCorTest(data = t.usrContribs[t.usrContribs$fstOr2ndhalf=='Second half',], x="WATER_LEVEL", y = "MeasuredWL")
spearTests1stHalf[ii] = SpearmanTest_1sthalf$estimate
spearTests2ndHalf[ii] = SpearmanTest_2ndhalf$estimate
}
# reverse the series to make it more intuitive
spearTests1stHalf = rev(spearTests1stHalf)
spearTests2ndHalf = rev(spearTests2ndHalf)
# bp = breakpoints(spearTests1stHalf~1)
# smoothing the data ----
windowSize = 5
smooth1stHalf = rollapply(spearTests1stHalf, width = windowSize, FUN = mean, align = "left")
smooth2ndHalf = rollapply(spearTests2ndHalf, width = windowSize, FUN = mean, align = "left")
# Check where the difference is biggest within the 10-90 percent of the data
smooth1stHalf_10t090 = smooth1stHalf[((length(smooth1stHalf)/10)*2):((length(smooth1stHalf)/10)*8)]
smooth2ndHalf_10t090 = smooth2ndHalf[((length(smooth2ndHalf)/10)*2):((length(smooth2ndHalf)/10)*8)]
absDifsOfSmooths = abs(smooth1stHalf-smooth2ndHalf)
which(max(absDifsOfSmooths,na.rm=T) == absDifsOfSmooths)
# code below only useful for testing with multiple splits for the dataset (default is 2 for splitting the data in half)
png(paste0('G:/h2k-data/Projects/CrowdWater/WP7 - CW Field and App Data Accuracy/Data/DataVisualisations/',stationsWithData$Stream[i],' - ',stationsWithData$Location[i],' User',tra,'.png'),width = 1024,height = 720)
plot(y=spearTests1stHalf,x=seq_along(spearTests1stHalf),col='orange',type='l',ylim=c(min(c(spearTests1stHalf,spearTests2ndHalf),na.rm = T),max(c(spearTests1stHalf,spearTests2ndHalf),na.rm = T)),main= paste0(stationsWithData$Stream[i],' - ',stationsWithData$Location[i],' [CrowdWater App]'),ylab= 'spearmans rho',xlab='Number of contribution [low numbers: almost no values are in the first part]')
lines(y=spearTests2ndHalf,x=seq_along(spearTests2ndHalf),col='lightblue')
lines(y=smooth1stHalf_10t090,x=seq_along(smooth1stHalf_10t090),col='red')
lines(y=smooth2ndHalf_10t090,x=seq_along(smooth2ndHalf_10t090),col='blue')
# abline(v=bp$breakpoints,col='darkgrey')
# text(labels=paste0('First BreakPoint - ', max(bp$breakpoints)),x =max(bp$breakpoints) ,y=mean(smooth1stHalf,na.rm = T))
legend(x='bottom',legend=c('1st half','1st half smooth','2nd half','2nd half smooth','Breakp. 1st H.'),fill = c('red','orange','blue','lightblue','darkgrey'))
dev.off()
t.usrContribs$WL4Plot = NA
t.usrContribs$WL4Plot[t.usrContribs$fstOr2ndhalf=='Second half'] = t.usrContribs$WLClasses_floor[t.usrContribs$fstOr2ndhalf=='Second half'] + 0.15
t.usrContribs$WL4Plot[t.usrContribs$fstOr2ndhalf=='First half'] = t.usrContribs$WLClasses_floor[t.usrContribs$fstOr2ndhalf=='First half'] - 0.15
# create the boxplot
png(paste0('G:/h2k-data/Projects/CrowdWater/WP7 - CW Field and App Data Accuracy/Data/DataVisualisations/Boxplot_WL_',stationsWithData$Stream[i],'_',stationsWithData$Location[i],'_CWApp.png'),width = 1024,height = 720)
print(
ggplot(data=t.usrContribs,aes(x=WL4Plot,y=MeasuredWL,fill=fstOr2ndhalf,group=WL4Plot))+
geom_boxplot())+
ggtitle(paste0(usersWithMoreThanEq10Contribs[tra],' - ',stationsWithData$Stream[i],' - ',stationsWithData$Location[i],' [CrowdWater App]'))
dev.off()
}
}
# split their contribution in half and do two additional boxplots for that person
rm(t.uq.classes,SpearmanTest,maxContribPerHour)
}
plot(y=spearTests1stHalf,x=seq_along(spearTests1stHalf),col='orange',type='l',ylim=c(min(c(spearTests1stHalf,spearTests2ndHalf),na.rm = T),max(c(spearTests1stHalf,spearTests2ndHalf),na.rm = T)),main= paste0(stationsWithData$Stream[i],' - ',stationsWithData$Location[i],' [CrowdWater App]'),ylab= 'spearmans rho',xlab='Number of contribution [low numbers: almost no values are in the first part]')
lines(y=spearTests2ndHalf,x=seq_along(spearTests2ndHalf),col='lightblue')
lines(y=smooth1stHalf_10t090,x=seq_along(smooth1stHalf_10t090),col='red')
lines(y=smooth2ndHalf_10t090,x=seq_along(smooth2ndHalf_10t090),col='blue')
absDifsOfSmooths = abs(smooth1stHalf-smooth2ndHalf)
which(max(absDifsOfSmooths,na.rm=T) == absDifsOfSmooths)
absDifsOfSmooths
absDifsOfSmooths = abs(smooth1stHalf_10t090-smooth2ndHalf_10t090)
which(max(absDifsOfSmooths,na.rm=T) == absDifsOfSmooths)
SplitSmootIdx = which(max(absDifsOfSmooths,na.rm=T) == absDifsOfSmooths)
SplitSmoothIdx = which(max(absDifsOfSmooths,na.rm=T) == absDifsOfSmooths)
SplitSmoothIdx
t.usrContribs
SpearmanTest_1sthalf
splitters
