CrossValMWPvalTab = as.data.frame(matrix(data=NA, ncol = length(uq.yearchar),nrow = length(st.uq.statGroup)))
colnames(CrossValMWPvalTab) = uq.yearchar
row.names(CrossValMWPvalTab) = st.uq.statGroup
for(j in seq_along(uq.yearchar)){
valYearChar = strsplit(uq.yearchar[j],'CalBy')[[1]][1]
calYearChar = strsplit(uq.yearchar[j],'CalBy')[[1]][2]
t.yc.CrossValidTab = CrossValidTab_Medians2p0[CrossValidTab_Medians2p0$yearChar==uq.yearchar[j] & CrossValidTab_Medians2p0$statGroup!='random_random_random',]
t.yc_Rand4CrossValTab = CrossValidTab_Medians2p0[CrossValidTab_Medians2p0$yearChar==paste0(valYearChar,'CalBy',calYearChar) & CrossValidTab_Medians2p0$statGroup == 'random_random_random',]
t.yc.CrossValidTab_withRandom = rbind(t.yc.CrossValidTab,t.yc_Rand4CrossValTab)
resList = list()
print(uq.yearchar[j])
for (i in seq_along(st.uq.statGroup)){
if(st.uq.statGroup[i] == 'random_random_random')next()
tab_lowBench = t.yc.CrossValidTab_withRandom[t.yc.CrossValidTab_withRandom$statGroup == 'random_random_random',]
tab_yearDat = t.yc.CrossValidTab_withRandom[t.yc.CrossValidTab_withRandom$statGroup == st.uq.statGroup[i],]
tab_lowBench_ord = as.data.frame(matrix(data=NA, ncol=ncol(tab_yearDat),nrow=nrow(tab_yearDat)))
colnames(tab_lowBench_ord) = colnames(tab_yearDat)
for(jj in seq_along(tab_yearDat$orderString)){
tab_lowBench_ord[jj,] = tab_lowBench[tab_lowBench$orderString==tab_yearDat$orderString[jj],]
}
resList[[i]] = wilcox.test(tab_yearDat$POA,tab_lowBench_ord$POA,mu=0, conf.int=T, conf.level=0.95, paired=T, exact =T,alternative = 'greater')
print(paste0(st.uq.statGroup[i],': p.value = ',round(resList[[i]]$p.value,3)))
CrossValMWPvalTab[i,j] = round(resList[[i]]$p.value,3)
}
}
# levelplot(as.matrix(CrossValMWPvalTab),scales=list(x=list(rot=45,cex=.75,draw=T)))
write.table(CrossValMWPvalTab,file=paste0(RoughPath,'CrossValMannWhitneyPvalTab_byYearChar.txt'),sep=',')
warnings()
resListDBt = list()
PlotList = list()
tab4Plotlist = list()
st.uq.quality4plot = c('no error','large error','medium error','small error')
letter4plot = c('a)','d)','c)','b)')
for (i in seq_along(st.uq.quality)){
for(j in seq_along(st.uq.class)){
if(st.uq.class[j]=="Streamflow" || st.uq.class[j] =="random"|| (st.uq.class[j] !='10ClassesCER' && is.element(st.uq.quality[i],c('SDdivBy1','SDdivBy2','SDdivBy4'))))next
stat.DBtestTab = CrossValidTab_Medians2p0[CrossValidTab_Medians2p0$NoiseType == st.uq.quality[i] & CrossValidTab_Medians2p0$Class == st.uq.class[j],]
stat.DBtestTab$TempRes = as.factor(stat.DBtestTab$TempRes)
x = stat.DBtestTab$rel2bench
y = stat.DBtestTab$TempRes
resListDBt[[i]]=dunn.test(x,y,kw=F,method = 'bonferroni',table = F, list=F,alpha=0.1)
# resListDBt[[i]]=dunn.test(x,y,kw=F,method = 'none') -> without adjusted p-values
# assessment of the strength of the effect
n = length(unique(y))
effectStrength = abs(resListDBt[[i]]$Z/sqrt(n))
print(paste0(st.uq.quality[i],' Group: D-B p.values'))
comparisons = t(as.data.frame(strsplit(resListDBt[[i]]$comparisons,' - ')))
resTab = as.data.frame(cbind(unname(comparisons),resListDBt[[i]]$P,resListDBt[[i]]$P.adjusted,effectStrength))
colnames(resTab)<- c('Comparison1','Comparison2','p-values','Adj.p-values','StrengthOfEffect')
idxs = order(resTab$`p-values`)
resTab = resTab[idxs,]
print(resTab)
p.nA.xtab = as.matrix(xtabs(as.numeric(as.character(resTab$`p-values`))~resTab$Comparison1+resTab$Comparison2))
p.Adj.xtab = as.matrix(xtabs(as.numeric(as.character(resTab$`Adj.p-values`))~resTab$Comparison1+resTab$Comparison2))
# reorder p.nA.xtab & p.Adj.xtab
newOrder = c('hourly','Daily','weekly','CWTempRes52','monthly','CWTempRes12','MarAugSatSun','MayOctSatSun','JAS2ndDay')
new.p.Adj.xtab = matrix(nrow = nrow(p.Adj.xtab),ncol = ncol(p.Adj.xtab))
colnames(new.p.Adj.xtab) = newOrder[2:length(newOrder)]
row.names(new.p.Adj.xtab) = newOrder[1:length(newOrder)-1]
p.Adj.xtab[p.Adj.xtab==0]<- NA
p.nA.xtab[p.nA.xtab==0]<- NA
n.cnames.adj = colnames(new.p.Adj.xtab)
n.rnames.adj = row.names(new.p.Adj.xtab)
meltedOldTab = melt(p.Adj.xtab)
meltedOldTab = meltedOldTab[!is.na(meltedOldTab$value),]
for(c in seq_along(n.cnames.adj)){
for(r in seq_along(n.rnames.adj)){
if(n.cnames.adj[c] == n.rnames.adj[r]){
next
}else{
valfromOldTab = meltedOldTab$value[xor((meltedOldTab$`resTab$Comparison1`== n.cnames.adj[c] & meltedOldTab$`resTab$Comparison2`== n.rnames.adj[r]),(meltedOldTab$`resTab$Comparison2`== n.cnames.adj[c] & meltedOldTab$`resTab$Comparison1`== n.rnames.adj[r]))]
new.p.Adj.xtab[r,c] = valfromOldTab
}
}
}
# remove the obsolete values
for(c in seq_along(n.cnames.adj)){
if(length(which(is.na(new.p.Adj.xtab[,c])))>0){
new.p.Adj.xtab[which(is.na(new.p.Adj.xtab[,c])):ncol(new.p.Adj.xtab),c] = NA
}
}
new.p.nA.xtab = matrix(nrow = nrow(p.nA.xtab),ncol = ncol(p.nA.xtab))
colnames(new.p.nA.xtab) = newOrder[2:length(newOrder)]
row.names(new.p.nA.xtab) = newOrder[1:length(newOrder)-1]
n.cnames.adj = colnames(new.p.nA.xtab)
n.rnames.adj = row.names(new.p.nA.xtab)
meltedOldTab = melt(p.nA.xtab)
meltedOldTab = meltedOldTab[!is.na(meltedOldTab$value),]
for(c in seq_along(n.cnames.adj)){
for(r in seq_along(n.rnames.adj)){
if(n.cnames.adj[c] == n.rnames.adj[r]){
next
}else{
valfromOldTab = meltedOldTab$value[xor((meltedOldTab$`resTab$Comparison1`== n.cnames.adj[c] & meltedOldTab$`resTab$Comparison2`== n.rnames.adj[r]),(meltedOldTab$`resTab$Comparison2`== n.cnames.adj[c] & meltedOldTab$`resTab$Comparison1`== n.rnames.adj[r]))]
new.p.nA.xtab[r,c] = valfromOldTab
}
}
}
# remove the obsolete values
for(c in seq_along(n.cnames.adj)){
if(length(which(is.na(new.p.nA.xtab[,c])))>0){
new.p.nA.xtab[which(is.na(new.p.nA.xtab[,c])):ncol(new.p.nA.xtab),c] = NA
}
}
# colors <- colorRampPalette(c("cornsilk","cornsilk4"))(length(seq(0.05,1,by=0.05)))
colors <- crameriPalette("oslo",length(seq(0.05,1,by=0.05)))
pointFill = "#FFFFFF"
starIdx = as.data.frame(which(new.p.Adj.xtab<=0.05,arr.ind = T))
starIdx_nA = as.data.frame(which(new.p.nA.xtab<=0.05,arr.ind = T))
p_symbol = 24
p_adj_symbol =25
# insert the new names
row.names(new.p.nA.xtab) = c('Hourly','Daily','Weekly','Crowd52','Monthly','Crowd12','WeekendSpring','WeekendSummer')
colnames(new.p.nA.xtab) = c('Daily','Weekly','Crowd52','Monthly','Crowd12','WeekendSpring','WeekendSummer','IntenseSummer')
setEPS()
postscript(paste0('//files.geo.uzh.ch/shared/group/h2k-data/Projects/CrowdWater/WP1.5-WaterLevelClasses/Paper/Figures/BonferroniHeatMaps/CrossValBonferroniHeatMap_',st.uq.quality[i],'_',st.uq.class[j],'.eps'),width = 7,height = 7)
# print(levelplot(new.p.nA.xtab,xlab ='', ylab='',aspect='iso',col.regions=colors,main = st.uq.quality4plot[i],
print(
levelplot(new.p.nA.xtab,xlab ='', ylab='',aspect='iso',col.regions=colors,main = paste0(st.uq.quality4plot[i],' - ',st.uq.class[j]),
colorkey = list(labels=list(labels = c('0','0.05','0.1','0.2','0.3','0.4','0.5'),at=c(0,0.05,0.1,0.2,0.3,0.4,0.5))),
scales=list(tck = c(1,0),
x=list(rot=45,cex=2,draw=T),#,draw=ifelse(i==1||i==4,F,T)),
y=list(cex=2,draw=T),#draw=ifelse(i==2||i==4,F,T)),
main=list(st.uq.quality4plot[i],cex=2)),
panel=function(...){             # for the points on the plot (Stars where p.adj <= 0.05)
panel.levelplot(...)
if(nrow(starIdx_nA)!=0){
grid.points(x= starIdx_nA$row,y=starIdx_nA$col, pch=p_symbol,gp=gpar(fill=pointFill))}
if(nrow(starIdx)!=0){
grid.points(x= starIdx$row,y=starIdx$col, pch=p_adj_symbol,gp=gpar(fill=pointFill))}
grid.points(x = 4.5 ,y = 1.00, pch=p_symbol,gp=gpar(fill=pointFill))
grid.points(x = 4.5 ,y = 1.60, pch=p_symbol,gp=gpar(fill=pointFill))
grid.points(x = 4.5 ,y = 1.00, pch=p_adj_symbol,gp=gpar(fill=pointFill))
grid.text('p',just='left',0.6,0.18)
grid.text(expression('p'[adj.]),just='left',0.60,0.08)
grid.text('< 0.05',just='left',0.71,0.18)
grid.text('< 0.05',just='left',0.71,0.08)
}
)
)
dev.off()
# panel.text(df$x, df$y, df$level, cex=0.5)
# print(st.uq.class[j])
# weekendSpringvsSummer = data.frame(case = length(st.uq.class),)
# new.p.nA.xtab['WeekendSpring',]['WeekendSummer'])
# new.p.nA.xtab['WeekendSpring',]['IntenseSummer'])
}
}
resListDBt[[i]]$P.adjusted[resListDBt[[i]]$comparisons=="hourly - weekly"]
resListDBt[[i]]$P[resListDBt[[i]]$comparisons=="hourly - weekly"]
# PlotLis_ord = PlotList[c(1,4,3,2)]
#
# png(filename = 'G:/h2k-data/Projects/CrowdWater/WP1/Paper/Figures/BonferroniHeatMap.png',width = 1000,height = 1000)
# c(PlotLis_ord[[1]],PlotLis_ord[[2]],PlotLis_ord[[3]],PlotLis_ord[[4]], x.same = NA, y.same = NA,
#   layout = NULL, merge.legends = FALSE, recursive = FALSE)
# dev.off()
#
#
# mergedPlots = c(PlotLis_ord[[1]],PlotLis_ord[[2]],PlotLis_ord[[3]],PlotLis_ord[[4]], x.same = NA, y.same = NA,
#   layout = NULL, merge.legends = FALSE, recursive = FALSE)
# update(mergedPlots,ylab.right = F)
# setEPS()
# postscript("G:/h2k-data/Projects/CrowdWater/WP1/Paper/Figures/BonferroniHeatMap.eps",width = 15,height = 15)
# mergedPlots
# dev.off()
#
#
# # Put the plots into one image
# PlotLis_ord = PlotList[c(1,4,3,2)]
# png(filename = 'G:/h2k-data/Projects/CrowdWater/WP1/Paper/Figures/BonferroniHeatMap.png',width = 1000,height = 1000)
# plot.new()
# par(mfrow=c(2,2), oma=c(2,0,2,0))
# print(PlotLis_ord[[1]], split=c(1, 1, 2, 2))
# print(PlotLis_ord[[2]], split=c(2, 1, 2, 2), newpage=FALSE)
# print(PlotLis_ord[[3]], split=c(1, 2, 2, 2), newpage=FALSE)
# print(PlotLis_ord[[4]], split=c(2, 2, 2, 2), newpage=FALSE)
# grid.text("a)", x = .1, y = .945, just = c("left", "bottom"), gp = gpar(cex=2))
# grid.text("b)", x = .6, y = .945, just = c("left", "bottom"), gp = gpar(cex=2))
# grid.text("c)", x = .1, y = .445, just = c("left", "bottom"), gp = gpar(cex=2))
# grid.text("d)", x = .6, y = .445, just = c("left", "bottom"), gp = gpar(cex=2))
# dev.off()
#
# c(PlotLis_ord[[1]],PlotLis_ord[[2]],PlotLis_ord[[3]],PlotLis_ord[[4]],layout=2:2)
# p1 = levelplot(p.nA.xtab)
# p2 = levelplot(p.nA.xtab)
# p3 = levelplot(p.nA.xtab)
# p4 = levelplot(p.nA.xtab)
# c(p1,p2,p3,p4,layout=2:2)
shiny::runApp()
locFile4Attempt = 'CW_Data.csv'
observeEvent(input$reloadAllCWdata,{
CWdataFull = Download_AllCWdata_from_API()
colnames(CWdataFull)[1]='Spot_ID'
write.csv(CWdataFull,file=paste0("CWdata/",locFile4Attempt),row.names = F)
js$refresh()
})
if(file.exists(paste0("CWdata/",locFile4Attempt))){
CWdataFull = read.csv(paste0("CWdata/",locFile4Attempt))
latestUpdate = CWdataFull$created_at[nrow(CWdataFull)]
newCWdata = Download_LatestCWdata_from_API(lastDate = latestUpdate)
if(!is.null(newCWdata)){
colnames(newCWdata)[1]='Spot_ID'
CWdataFull = rbind(CWdataFull,newCWdata)
write.csv(CWdataFull,file=paste0("CWdata/",locFile4Attempt),row.names = F)
}
}else{
CWdataFull = Download_AllCWdata_from_API()
colnames(CWdataFull)[1]='Spot_ID'
write.csv(CWdataFull,file=paste0("CWdata/",locFile4Attempt),row.names = F)
}
CWdata = CWdataFull # select all CW data
output$allRootIDs = unique(CWdata$ROOTID)
runApp()
shiny::runApp()
TS_LUT <<- data.frame(TSID = 471:476,TSnr = seq_along(471:476),TSInput=c("dry streambed","damp / wet streambed","isolated pools","trickling water","standing water","flowing"))
TS_LUT
source('G:/h2k-data/Projects/CrowdWater/App & Homepage/Homepage/DataDashboard/CrowdWaterDashboard/CW_API_Download.R', echo=TRUE)
source('G:/h2k-data/Projects/CrowdWater/App & Homepage/Homepage/DataDashboard/CrowdWaterDashboard/CW_API_Download.R', echo=TRUE)
runApp()
source('G:/h2k-data/Projects/CrowdWater/App & Homepage/Homepage/DataDashboard/CrowdWaterDashboard/CW_API_Download.R', echo=TRUE)
source('G:/h2k-data/Projects/CrowdWater/App & Homepage/Homepage/DataDashboard/CrowdWaterDashboard/CW_API_Download.R', echo=TRUE)
runApp()
shiny::runApp()
runApp()
#  Function to combine leaflet with a static probability density function from the CW points
addHeatMap <- function(data, lon, lat, intensity, ...) {
df <- data.table::as.data.table(data)
df_expanded <- dplyr::slice(df, rep(1:dplyr::n(), times = !! enquo(intensity)))
lon_var <- dplyr::pull(df_expanded, !! enquo(lon))
lat_var <- dplyr::pull(df_expanded, !! enquo(lat))
lon_bw <- MASS::bandwidth.nrd(lon_var)
lat_bw <- MASS::bandwidth.nrd(lat_var)
lon_lat_df <- dplyr::select(df_expanded, !! enquo(lon), !! enquo(lat))
kde <- KernSmooth::bkde2D(lon_lat_df, bandwidth = c(lon_bw, lat_bw))
CL <- contourLines(kde$x1 , kde$x2 , kde$fhat)
LEVS <- as.factor(sapply(CL, `[[`, "level"))
NLEV <- nlevels(LEVS)
pgons <- lapply(1:length(CL), function(i)
sp::Polygons(list(sp::Polygon(cbind(CL[[i]]$x, CL[[i]]$y))), ID = i))
spgons <- sp::SpatialPolygons(pgons)
leaflet::addPolygons(data = spgons, color = heat.colors(NLEV, NULL)[LEVS], stroke = FALSE, ...)
}
locFile4Attempt = 'CW_Data.csv'
observeEvent(input$reloadAllCWdata,{
CWdataFull = Download_AllCWdata_from_API()
colnames(CWdataFull)[1]='Spot_ID'
write.csv(CWdataFull,file=paste0("CWdata/",locFile4Attempt),row.names = F)
js$refresh()
})
if(file.exists(paste0("CWdata/",locFile4Attempt))){
CWdataFull = read.csv(paste0("CWdata/",locFile4Attempt))
latestUpdate = CWdataFull$created_at[nrow(CWdataFull)]
newCWdata = Download_LatestCWdata_from_API(lastDate = latestUpdate)
if(!is.null(newCWdata)){
colnames(newCWdata)[1]='Spot_ID'
CWdataFull = rbind(CWdataFull,newCWdata)
write.csv(CWdataFull,file=paste0("CWdata/",locFile4Attempt),row.names = F)
}
}else{
CWdataFull = Download_AllCWdata_from_API()
colnames(CWdataFull)[1]='Spot_ID'
write.csv(CWdataFull,file=paste0("CWdata/",locFile4Attempt),row.names = F)
}
CWdata = CWdataFull # select all CW data
# Base data manipulations for dashboard ----
CWdata$created_at = as.POSIXlt(CWdata$created_at,format = '%Y-%m-%d %H:%M:%S',tz='GMT',usetz=T)
uq.dates = unique(CWdata$created_at)
uq.roots = unique(CWdata$root_id)
uq.users = unique(CWdata$spotted_by)
# the start date is approximately when the app was officially launched, there are some pics from earlier dates that were added manually by us
dateSeries = seq(from=min(as.POSIXct("2017-01-01 01:00:00 GMT")),to=max(uq.dates)+3600,by='1 day')
attr(dateSeries,"tzone") = 'GMT'
View(CWdata)
source('G:/h2k-data/Projects/CrowdWater/App & Homepage/Homepage/DataDashboard/CrowdWaterDashboard/CW_API_Download.R', echo=TRUE)
runApp()
leaflet() %>%
addTiles(group = "OSM") %>%
addHeatMap(data = CWdata, lon = longitude, lat = latitude)
leaflet() %>%
addTiles(group = "OSM") %>%
addHeatMap(data = CWdata, lon = longitude, lat = latitude)
addHeatMap(data = CWdata, lon = longitude, lat = latitude)
source('G:/h2k-data/Projects/CrowdWater/App & Homepage/Homepage/DataDashboard/CrowdWaterDashboard/CW_API_Download.R', echo=TRUE)
leaflet() %>%
addTiles(group = "OSM") %>%
addHeatMap_kernel(data = CWdata, lon = longitude, lat = latitude)
runApp()
data = CWdata
lon = longitude
lat = latitude
df <- data.table::as.data.table(data)
data
data = CWdata
locFile4Attempt = 'CW_Data.csv'
observeEvent(input$reloadAllCWdata,{
CWdataFull = Download_AllCWdata_from_API()
colnames(CWdataFull)[1]='Spot_ID'
write.csv(CWdataFull,file=paste0("CWdata/",locFile4Attempt),row.names = F)
js$refresh()
})
if(file.exists(paste0("CWdata/",locFile4Attempt))){
CWdataFull = read.csv(paste0("CWdata/",locFile4Attempt))
latestUpdate = CWdataFull$created_at[nrow(CWdataFull)]
newCWdata = Download_LatestCWdata_from_API(lastDate = latestUpdate)
if(!is.null(newCWdata)){
colnames(newCWdata)[1]='Spot_ID'
CWdataFull = rbind(CWdataFull,newCWdata)
write.csv(CWdataFull,file=paste0("CWdata/",locFile4Attempt),row.names = F)
}
}else{
CWdataFull = Download_AllCWdata_from_API()
colnames(CWdataFull)[1]='Spot_ID'
write.csv(CWdataFull,file=paste0("CWdata/",locFile4Attempt),row.names = F)
}
CWdata = CWdataFull # select all CW data
# Base data manipulations for dashboard ----
CWdata$created_at = as.POSIXlt(CWdata$created_at,format = '%Y-%m-%d %H:%M:%S',tz='GMT',usetz=T)
uq.dates = unique(CWdata$created_at)
uq.roots = unique(CWdata$root_id)
uq.users = unique(CWdata$spotted_by)
# the start date is approximately when the app was officially launched, there are some pics from earlier dates that were added manually by us
dateSeries = seq(from=min(as.POSIXct("2017-01-01 01:00:00 GMT")),to=max(uq.dates)+3600,by='1 day')
attr(dateSeries,"tzone") = 'GMT'
addHeatMap_kernel(data = CWdata, lon = longitude, lat = latitude)
data = CWdata
lon = longitude
lat = latitude
df <- data.table::as.data.table(data)
df_expanded <- dplyr::slice(df, rep(1:dplyr::n()))
df_expanded
lon_var <- dplyr::pull(df, !! enquo(lon))
lat_var <- dplyr::pull(df, !! enquo(lat))
lon
lon=longitude
#  Function to combine leaflet with a static probability density function from the CW points
addHeatMap_kernel <- function(data, lon, lat, ...) {
df <- data.table::as.data.table(data)
lon_var <- dplyr::pull(df, !! enquo(lon))
lat_var <- dplyr::pull(df, !! enquo(lat))
lon_bw <- MASS::bandwidth.nrd(lon_var)
lat_bw <- MASS::bandwidth.nrd(lat_var)
lon_lat_df <- dplyr::select(df_expanded, !! enquo(lon), !! enquo(lat))
kde <- KernSmooth::bkde2D(lon_lat_df, bandwidth = c(lon_bw, lat_bw))
CL <- contourLines(kde$x1 , kde$x2 , kde$fhat)
LEVS <- as.factor(sapply(CL, `[[`, "level"))
NLEV <- nlevels(LEVS)
pgons <- lapply(1:length(CL), function(i)
sp::Polygons(list(sp::Polygon(cbind(CL[[i]]$x, CL[[i]]$y))), ID = i))
spgons <- sp::SpatialPolygons(pgons)
leaflet::addPolygons(data = spgons, color = heat.colors(NLEV, NULL)[LEVS], stroke = FALSE, ...)
}
leaflet() %>%
addTiles(group = "OSM") %>%
addHeatMap_kernel(data = CWdata, lon = longitude, lat = latitude)
#  Function to combine leaflet with a static probability density function from the CW points
addHeatMap_kernel <- function(data, lon, lat, ...) {
df <- data.table::as.data.table(data)
lon_var <- dplyr::pull(df, !! enquo(lon))
lat_var <- dplyr::pull(df, !! enquo(lat))
lon_bw <- MASS::bandwidth.nrd(lon_var)
lat_bw <- MASS::bandwidth.nrd(lat_var)
lon_lat_df <- dplyr::select(df, !! enquo(lon), !! enquo(lat))
kde <- KernSmooth::bkde2D(lon_lat_df, bandwidth = c(lon_bw, lat_bw))
CL <- contourLines(kde$x1 , kde$x2 , kde$fhat)
LEVS <- as.factor(sapply(CL, `[[`, "level"))
NLEV <- nlevels(LEVS)
pgons <- lapply(1:length(CL), function(i)
sp::Polygons(list(sp::Polygon(cbind(CL[[i]]$x, CL[[i]]$y))), ID = i))
spgons <- sp::SpatialPolygons(pgons)
leaflet::addPolygons(data = spgons, color = heat.colors(NLEV, NULL)[LEVS], stroke = FALSE, ...)
}
source('G:/h2k-data/Projects/CrowdWater/App & Homepage/Homepage/DataDashboard/CrowdWaterDashboard/CW_API_Download.R', echo=TRUE)
leaflet() %>%
addTiles(group = "OSM") %>%
addHeatMap_kernel(data = CWdata, lon = longitude, lat = latitude)
runApp()
runApp()
shiny::runApp()
locFile4Attempt = 'CW_Data.csv'
runApp()
runApp()
locFile4Attempt = 'CW_Data.csv'
observeEvent(input$reloadAllCWdata,{
CWdataFull = Download_AllCWdata_from_API()
colnames(CWdataFull)[1]='Spot_ID'
write.csv(CWdataFull,file=paste0("CWdata/",locFile4Attempt),row.names = F)
js$refresh()
})
if(file.exists(paste0("CWdata/",locFile4Attempt))){
CWdataFull = read.csv(paste0("CWdata/",locFile4Attempt))
latestUpdate = CWdataFull$created_at[nrow(CWdataFull)]
newCWdata = Download_LatestCWdata_from_API(lastDate = latestUpdate)
if(!is.null(newCWdata)){
colnames(newCWdata)[1]='Spot_ID'
CWdataFull = rbind(CWdataFull,newCWdata)
write.csv(CWdataFull,file=paste0("CWdata/",locFile4Attempt),row.names = F)
}
}else{
CWdataFull = Download_AllCWdata_from_API()
colnames(CWdataFull)[1]='Spot_ID'
write.csv(CWdataFull,file=paste0("CWdata/",locFile4Attempt),row.names = F)
}
CWdata = CWdataFull # select all CW data
library(shiny)
library(shinydashboard)
library(ggplot2)
library(dplyr)
library(shinycssloaders)
library(shinyjs)
library(rdrop2)
library(V8)
library(leaflet)
library(leaflet.extras)
library(htmltools)
# setwd('G:/h2k-data/Projects/CrowdWater/App & Homepage/Homepage/DataDashboard/CrowdWaterDashboard/')
# setwd('G:/group/h2k-data/Projects/CrowdWater/App & Homepage/Homepage/DataDashboard/CrowdWaterDashboard')
source('./CW_API_Download.R')
# icons: http://rstudio.github.io/shinydashboard/appearance.html
# the javascript code to refresh the entire page.
jscode <- "shinyjs.refresh = function() { history.go(0); }"
locFile4Attempt = 'CW_Data.csv'
observeEvent(input$reloadAllCWdata,{
CWdataFull = Download_AllCWdata_from_API()
colnames(CWdataFull)[1]='Spot_ID'
write.csv(CWdataFull,file=paste0("CWdata/",locFile4Attempt),row.names = F)
js$refresh()
})
if(file.exists(paste0("CWdata/",locFile4Attempt))){
CWdataFull = read.csv(paste0("CWdata/",locFile4Attempt))
latestUpdate = CWdataFull$created_at[nrow(CWdataFull)]
newCWdata = Download_LatestCWdata_from_API(lastDate = latestUpdate)
if(!is.null(newCWdata)){
colnames(newCWdata)[1]='Spot_ID'
CWdataFull = rbind(CWdataFull,newCWdata)
write.csv(CWdataFull,file=paste0("CWdata/",locFile4Attempt),row.names = F)
}
}else{
CWdataFull = Download_AllCWdata_from_API()
colnames(CWdataFull)[1]='Spot_ID'
write.csv(CWdataFull,file=paste0("CWdata/",locFile4Attempt),row.names = F)
}
CWdata = CWdataFull # select all CW data
# Base data manipulations for dashboard ----
CWdata$created_at = as.POSIXlt(CWdata$created_at,format = '%Y-%m-%d %H:%M:%S',tz='GMT',usetz=T)
uq.dates = unique(CWdata$created_at)
uq.roots = unique(CWdata$root_id)
uq.users = unique(CWdata$spotted_by)
# the start date is approximately when the app was officially launched, there are some pics from earlier dates that were added manually by us
dateSeries = seq(from=min(as.POSIXct("2017-01-01 01:00:00 GMT")),to=max(uq.dates)+3600,by='1 day')
attr(dateSeries,"tzone") = 'GMT'
# cumsums at dates and Root IDs with corresponding updates
cumSums = sapply(dateSeries,function(x) length(CWdata$Spot_ID[CWdata$created_at<=x]))
cumSumUsers = sapply(dateSeries,function(x) length(unique(CWdata$spotted_by[CWdata$created_at<=x])))
IdsPerRoot = sapply(uq.roots,function(x) CWdata$Spot_ID[CWdata$root_id==x])
IdsPerUser = sapply(uq.users,function(x) CWdata$Spot_ID[CWdata$spotted_by==x])
# for the slider further below
maxcontribs = max(sapply(IdsPerRoot, function(x) length(x)))
maxcontribUser = max(sapply(IdsPerUser, function(x) length(x)))
# for temporary streams
CWdataTS = CWdata[CWdata$category==468,]
# for water levels
CWdataWL = CWdata[CWdata$category==470,]
IdsPerRootWL = sapply(uq.roots,function(x) CWdataWL$Spot_ID[CWdataWL$root_id==x])
IdsPerUserWL = sapply(uq.users,function(x) CWdataWL$Spot_ID[CWdataWL$spotted_by==x])
IdsPerRootWL
uq.datesWL = unique(CWdataWL$created_at)
dateSeriesWL = seq(from=min(as.POSIXct("2017-01-01 01:00:00 GMT")),to=max(uq.datesWL)+3600,by='1 day')
cumSumsWL = sapply(dateSeriesWL,function(x) length(CWdataWL$Spot_ID[CWdataWL$created_at<=x]))
cumSumsUsersWL = sapply(dateSeriesWL,function(x) length(unique(CWdataWL$spotted_by[CWdataWL$created_at<=x])))
length(unique(CWdataWL$root_id))
require(xlsx)
data = read.xlsx2("G:\\h2k-data\\Projects\\CrowdWater\\WP7 - CW Field and App Data Accuracy\\Data\\ListWithCoordinates_WGS84.xlsx",1,stringsAsFactors=F)
data$lonWLmeasured = as.numeric(data$Coordinates.WL.measurements..E.)
data$latWLmeasured = as.numeric(data$Coordinates.WL.measurements..N.)
data$lonWLObs = as.numeric(data$Coordinates.WL.class.observations..E..)
data$latWLObs= as.numeric(data$Coordinates.WL.class.observations..N..)
require(geosphere)
data$distance_km= round(sapply(1:nrow(data), function(x){
distm(c(data$lonWLmeasured[x],data$latWLmeasured[x]),
c(data$lonWLObs[x],data$latWLObs[x]),fun=distHaversine)
})/1000,1)
data$distance_km
View(data)
medianDist = median(data$distance_km)
medianDist
data
source('G:/h2k-data/Projects/CrowdWater/WP7 - CW Field and App Data Accuracy/Rscripts/CrowdWaterDataQuality/DistanceBetweenPoints.R', echo=TRUE)
require(xlsx)
data = read.xlsx2("G:\\h2k-data\\Projects\\CrowdWater\\WP7 - CW Field and App Data Accuracy\\Data\\ListWithCoordinates_WGS84.xlsx",1,stringsAsFactors=F)
data$lonWLmeasured = as.numeric(data$Coordinates.WL.measurements..E.)
data$latWLmeasured = as.numeric(data$Coordinates.WL.measurements..N.)
data$lonWLObs = as.numeric(data$Coordinates.WL.class.observations..E..)
data$latWLObs= as.numeric(data$Coordinates.WL.class.observations..N..)
require(geosphere)
data$distance_km= round(sapply(1:nrow(data), function(x){
distm(c(data$lonWLmeasured[x],data$latWLmeasured[x]),
c(data$lonWLObs[x],data$latWLObs[x]),fun=distHaversine)
})/1000,1)
medianDist = median(data$distance_km)
plot(data$Kendall.s.t.,data$distance_km)
cor(data$Kendall.s.t.,data$distance_km, u method="pearson")
cor(data$Kendall.s.t.,data$distance_km, method="pearson")
data$Kendall.s.t.
cor(as.numeric(data$Kendall.s.t.),as.numeric(data$distance_km), method="pearson")
grepl(data$Number,'A')
data$Number
grepl('A',data$Number)
cor(as.numeric(data$Kendall.s.t.[grepl('A',data$Number)]),as.numeric(data$distance_km[grepl('A',data$Number)]), method="pearson")
plot(as.numeric(data$Kendall.s.t.[grepl('A',data$Number)]),as.numeric(data$distance_km[grepl('A',data$Number)]))
plot(as.numeric(data$Kendall.s.t.[grepl('P',data$Number)]),as.numeric(data$distance_km[grepl('P',data$Number)]))
cor(as.numeric(data$Kendall.s.t.[grepl('P',data$Number)]),as.numeric(data$distance_km[grepl('P',data$Number)]), method="pearson")
cor(as.numeric(data$Kendall.s.t.[grepl('c',data$Number)]),as.numeric(data$distance_km[grepl('P',data$Number)]), method="pearson")
